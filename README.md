# Undo/Redo Performance Benchmark

这个项目旨在比较不同不可变性实现方案在大规模数据结构下进行 Undo/Redo 操作的性能表现。

本项目及其文档使用 Cursor 调用 Claude-Sonnet 大模型生成。

## 背景

在现代前端应用中，Undo/Redo 功能变得越来越普遍。然而，当处理大规模数据结构时（例如包含数万行的复杂对象），如何高效地实现这个功能成为一个挑战。本项目通过对比多种流行的实现方案，帮助开发者在实际应用中做出更好的技术选择。

## 测试方案

### 测试数据结构
生成一个包含大量嵌套数据的测试集：
- 多层嵌套的对象结构
- 包含大量数组
- 深层属性
- 模拟真实应用场景的数据复杂度

### 测试实现方案
1. **Lodash Deep Clone**
   - 使用 `cloneDeep` 进行深拷贝
   - 完整复制整个状态树
   - 作为基准测试方案

2. **JSON Serialization**
   - 使用 `JSON.stringify/parse` 
   - 通过序列化实现深拷贝
   - 常见的简单实现方案

3. **Klona**
   - 高性能深拷贝库
   - 优化的克隆策略
   - 专注于性能的实现

4. **Immer**
   - 基于 Proxy 的不可变更新
   - 提供可变式的 API
   - 内部实现写时复制（CoW）

5. **Immutable.js**
   - 持久化数据结构
   - 结构共享
   - 高效的不可变操作

6. **Native Immutable**
   - 基于 `Object.freeze` 的实现
   - 使用展开运算符进行复制
   - 原生 JavaScript 实现

### 测试场景
1. **添加操作**
   - 向大型集合添加新元素
   - 测试结构扩展性能

2. **删除操作**
   - 从大型集合移除元素
   - 测试结构收缩性能

3. **深度修改**
   - 修改深层嵌套属性
   - 测试局部更新性能

## 实现细节

### 历史管理器设计
所有实现都遵循相同的接口：

```typescript
interface HistoryManager<T> {
  getCurrentState(): T;
  push(state: T): void;
  undo(): T | null;
  redo(): T | null;
  canUndo(): boolean;
  canRedo(): boolean;
}
```

### 关键优化策略

1. **Immutable.js 优化**
   - 避免 `toJS()/fromJS()` 转换
   - 保持数据在不可变形态
   - 利用结构共享

2. **Native Immutable 优化**
   - 使用 `Object.freeze` 确保不可变性
   - 最小化复制路径
   - 实现写时复制策略

3. **内存管理**
   - 实现垃圾回收辅助
   - 优化内存占用
   - 防止内存泄漏

### 性能考量

1. **写时复制 (CoW)**
   - 仅在必要时创建副本
   - 共享未修改的结构
   - 优化内存使用

2. **结构共享**
   - 复用未修改的节点
   - 减少内存占用
   - 提高比较效率

3. **路径优化**
   - 最小化复制路径
   - 局部更新策略
   - 减少不必要的对象创建

## 使用方法

1. **运行测试**
```typescript
const benchmarkService = new BenchmarkService();
await benchmarkService.runBenchmarks();
```

2. **查看结果**
- 控制台输出各实现方案的性能数据
- 包含添加、删除、修改操作的耗时
- 可比较不同方案的内存使用

## 注意事项

1. **数据规模**
   - 可通过 `generateLargeData` 调整测试数据量
   - 默认生成适度规模的测试数据
   - 可根据需要调整数据复杂度

2. **内存考量**
   - 大规模测试可能需要较大内存
   - 建议在开发环境进行测试
   - 注意监控内存使用情况

3. **测试环境**
   - 建议在 Chrome 开发者工具中进行测试
   - 可使用 Performance 面板分析详细性能数据
   - 考虑禁用垃圾回收对测试的影响

## 内存使用对比

### 各方案内存特征

1. **Lodash Deep Clone**
   - 每次操作都创建完整的数据副本
   - 内存使用随数据规模线性增长
   - 垃圾回收压力大
   - 适用场景：数据规模小，操作频率低

2. **JSON Serialization**
   - 序列化过程需要额外的字符串内存
   - 反序列化时创建新对象
   - 内存峰值较高
   - 适用场景：需要序列化存储的场景

3. **Klona**
   - 优化的深拷贝实现
   - 比 Lodash 更低的内存占用
   - 更快的内存释放
   - 适用场景：需要高性能深拷贝的场景

4. **Immer**
   - 基于代理的写时复制
   - 仅复制修改路径
   - 较低的内存开销
   - 适用场景：频繁的小规模修改

5. **Immutable.js**
   - 持久化数据结构
   - 高效的结构共享
   - 稳定的内存使用模式
   - 适用场景：大规模数据的频繁修改

6. **Native Immutable**
   - 基于原生 JavaScript 的写时复制
   - 适中的内存使用
   - 简单直观的内存模型
   - 适用场景：中等规模数据的一般操作

### 内存使用模式

#### 添加操作
- Immutable.js 和 Immer 表现最好，因为它们能够共享未修改的结构
- JSON 和 Lodash 的内存使用最高，因为需要完整复制
- Native Immutable 和 Klona 处于中间水平

#### 删除操作
- 所有方案的内存使用相对添加操作都较低
- Immutable.js 的内存释放最高效
- 其他方案的表现相近

#### 深度修改
- Immutable.js 和 Immer 因为路径共享而表现最好
- JSON 和 Lodash 需要完整复制，内存使用最高
- Native Immutable 因为需要复制修改路径，内存使用适中

### 内存优化建议

1. **选择合适的实现**
   - 小数据量：可以使用简单的深拷贝
   - 大数据量：推荐使用 Immutable.js 或 Immer
   - 中等数据量：可以考虑 Native Immutable

2. **内存管理策略**
   - 及时清理不需要的历史记录
   - 限制历史记录数量
   - 考虑使用内存池
   - 注意垃圾回收时机

3. **监控和优化**
   - 监控内存使用趋势
   - 识别内存泄漏
   - 优化数据结构
   - 合理设置更新频率

### 测试结果示例

## 结论

不同实现方案各有优势：
- Immutable.js：适合大规模数据的频繁更新
- Native Immutable：适合中小规模数据的简单操作
- Immer：适合需要可变式 API 的场景
- 其他方案各有特定使用场景

选择合适的实现方案应考虑：
1. 数据规模和复杂度
2. 操作频率和类型
3. 内存限制
4. 开发便利性
5. 团队熟悉度

## 贡献

欢迎提交 Issue 和 Pull Request 来改进这个基准测试项目。
